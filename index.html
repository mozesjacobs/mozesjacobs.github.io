---
layout: default
---

<p style="margin-bottom: 10px;">
  <a href="Resume.pdf" target="_blank">CV</a> |
  <a href="https://www.linkedin.com/in/mozesjacobs/" target="_blank">LinkedIn</a> |
  <a href="mailto:mozesjacobs@g.harvard.edu">Gmail</a>
</p>

<h2>About</h2>
<p>Hello! I am a third-year computer science PhD candidate at Harvard University,
  advised by Professor <a href="http://www.demba-ba.org">Demba Ba</a>. I am supported by the
  <a href="https://www.harvard.edu/kempner-institute/opportunities/the-kempner-institute-graduate-fellowship/">Kempner
    Institute Graduate Fellowship</a>. <br> <br>

  I'm interested in <strong>Visual Representation Learning</strong> and <strong>Interpretability</strong>.<br><br>

  My work investigates how large-scale vision models transform high-dimensional visual inputs into structured semantic
  representations.
  I'm curious about how mechanistic insights into model computations can go beyond analysis to inform the design of
  effective structural inductive biases.
  I am particularly interested in how architectural constraints compatible with scaling —
  such as recurrence — can be engineered to improve the efficiency, robustness, generalization capabilities of
  models.<br> <br>

  Previously, I worked at the <a href="https://dynamicsai.org">AI Institute in Dynamic Systems</a> with
  <a href="https://amath.washington.edu/people/j-nathan-kutz">Nathan Kutz</a> and
  <a href="https://ryraut.github.io">Ryan Raut</a>.
  I earned my B.S. in computer science from the <a href="https://www.cs.washington.edu">Allen School</a> at the
  <a href="https://www.washington.edu">University of Washington</a>, where I worked with <a
    href="https://www.rajeshpnrao.com">Rajesh Rao</a>
  and <a href="https://noble.gs.washington.edu/~wnoble/">William Noble</a>.
</p>


<h2>Publications and Preprints</h2>

<ol>
  <li>
    <strong>Jacobs M.*</strong>, Fel T.*, Hakim R.*, Brondetta A., Ba D., Keller TA. (2025).<br>
    <a href="https://arxiv.org/abs/2512.19941">
      <cite>Block-Recurrent Dynamics in ViTs</cite>
    </a><br>
    <em>Accepted to ICLR 2026</em>.
    <p>
      <img src="raptor.png" alt="Raptor" style="width: 8cm; height: auto; margin: 10px 0;">
      We introduce the Block-Recurrent Hypothesis (BRH), arguing that trained ViTs admit a block-recurrent depth
      structure.
      To validate this, we train recurrent surrogates called Raptor. We demonstrate that a Raptor model can recover
      <strong>96% of DINOv2 ImageNet-1k linear probe accuracy in only 2 blocks</strong> while maintaining equivalent
      computational cost.
    </p>
  </li>
  <li>
    <strong>Jacobs M.</strong>, Budzinski RC., Muller L., Ba D., Keller TA. (2025).<br>
    <a href="https://openreview.net/pdf?id=QEzqo546V5">
      <cite>Traveling Waves Integrate Spatial Information Through Time</cite>
    </a><br>
    <em>CCN 2025</em>.
    <p>
      <img src="traveling_waves.gif" alt="Traveling Waves" style="max-width: 100%; height: auto; margin: 10px 0;">
    </p>
  </li>
  <li>
    <strong>Jacobs M.</strong>, Budzinski RC., Muller L., Ba D., Keller TA. (2025).<br>
    <a href="https://arxiv.org/abs/2502.06034">
      <cite>Traveling Waves Integrate Spatial Information Into Spectral Representations</cite>
    </a><br>
    <em>ICLR 2025 Re-Align Workshop</em>.
  </li>
  <li>
    <strong>Jacobs M.</strong>, Brunton BW., Brunton SL., Kutz JN., Raut RV. (2023).<br>
    <a href="https://arxiv.org/abs/2310.04832">
      <cite>HyperSINDY: Deep Generative Modeling of Nonlinear Stochastic Governing Equations</cite>
    </a>
  </li>
  <li>
    <strong>Jacobs M.</strong>, Jiang LP., Rao RP. (2022).<br>
    <a href="https://mozesjacobs.github.io/gopc.pdf">
      <cite>Gradient Origin Predictive Coding</cite>
    </a><br>
    Undergraduate senior thesis.
  </li>
</ol>
